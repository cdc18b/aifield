{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md413FzAvFD8"
      },
      "source": [
        "# DX 704 Week 6 Project\n",
        "\n",
        "This project will develop a treatment plan for a fictious illness \"Twizzleflu\".\n",
        "Twizzleflu is a mild illness caused by a virus.\n",
        "The main symptoms are a mild fever, fidgeting, and kicking the blankets off the bed or couch.\n",
        "Mild dehydration has also been reported in more severe cases.\n",
        "These symptoms typically last 1-2 weeks without treatment.\n",
        "Word on the internet says that Twizzleflu can be cured faster by drinking copious orange juice, but this has not been supported by evidence so far.\n",
        "You will be provided with a theoretical model of Twizzleflu modeled as a Markov decision process.\n",
        "Based on the model, you will compute optimal treatment plans to optimize different criteria, and compare patient discomfort with the different plans."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzyRo9Tw5VcB"
      },
      "source": [
        "The full project description, a template notebook, and raw data are available on GitHub: [Project 6 Materials](https://github.com/bu-cds-dx704/dx704-project-06)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGYOZcnP6Vfu"
      },
      "source": [
        "We will model Twizzleflu as a Markov decision process.\n",
        "The model transition probabilities are provided in the file \"twizzleflu-transitions.tsv\" and the expected rewards are in \"twizzleflu-rewards.tsv\".\n",
        "The goal for Twizzleflu is to minimize the expected discomfort of the patient which is expressed as negative rewards in the file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlm2sUsades5"
      },
      "source": [
        "## Example Code\n",
        "\n",
        "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
        "\n",
        "* https://github.com/bu-cds-omds/dx601-examples\n",
        "* https://github.com/bu-cds-omds/dx602-examples\n",
        "* https://github.com/bu-cds-omds/dx603-examples\n",
        "* https://github.com/bu-cds-omds/dx704-examples\n",
        "\n",
        "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "rewards_df = pd.read_csv(\"twizzleflu-rewards.tsv\", sep=\"\\t\")\n",
        "transitions_df = pd.read_csv(\"twizzleflu-transitions.tsv\", sep=\"\\t\")\n",
        "\n",
        "def build_lookups(rewards_table, transitions_table):\n",
        "    all_states = set(rewards_table[\"state\"].tolist())\n",
        "    all_states.update(transitions_table[\"state\"].tolist())\n",
        "    all_states.update(transitions_table[\"next_state\"].tolist())\n",
        "\n",
        "    all_actions = sorted(set(rewards_table[\"action\"].tolist()) | set(transitions_table[\"action\"].tolist()))\n",
        "\n",
        "    reward_by_state_action = {}\n",
        "    for _, row in rewards_table.iterrows():\n",
        "        reward_by_state_action[(row[\"state\"], row[\"action\"])] = float(row[\"reward\"])\n",
        "\n",
        "    transitions_by_state_action = {}\n",
        "    for _, row in transitions_table.iterrows():\n",
        "        key = (row[\"state\"], row[\"action\"])\n",
        "        transitions_by_state_action.setdefault(key, []).append((row[\"next_state\"], float(row[\"probability\"])))\n",
        "\n",
        "    return all_states, all_actions, reward_by_state_action, transitions_by_state_action\n",
        "\n",
        "\n",
        "def get_available_actions(state, all_actions, reward_by_state_action, transitions_by_state_action):\n",
        "    available = []\n",
        "    for action in all_actions:\n",
        "        if (state, action) in reward_by_state_action or (state, action) in transitions_by_state_action:\n",
        "            available.append(action)\n",
        "    return available\n",
        "\n",
        "\n",
        "def run_value_iteration_optimal(all_states, all_actions, reward_by_state_action, transitions_by_state_action):\n",
        "    value_by_state = {state: 0.0 for state in all_states}\n",
        "\n",
        "    max_iterations = 200000\n",
        "    tolerance = 0.0000000001\n",
        "\n",
        "    for _ in range(max_iterations):\n",
        "        biggest_change = 0.0\n",
        "        new_value_by_state = {}\n",
        "\n",
        "        for state in all_states:\n",
        "            available_actions = get_available_actions(state, all_actions, reward_by_state_action, transitions_by_state_action)\n",
        "\n",
        "            if len(available_actions) == 0:\n",
        "                best_value = 0.0\n",
        "            else:\n",
        "                best_value = None\n",
        "                for action in available_actions:\n",
        "                    immediate_reward = float(reward_by_state_action.get((state, action), 0.0))\n",
        "\n",
        "                    expected_future_value = 0.0\n",
        "                    for next_state, probability in transitions_by_state_action.get((state, action), []):\n",
        "                        expected_future_value += probability * value_by_state[next_state]\n",
        "\n",
        "                    q_value = immediate_reward + expected_future_value\n",
        "\n",
        "                    if best_value is None or q_value > best_value:\n",
        "                        best_value = q_value\n",
        "\n",
        "            new_value_by_state[state] = float(best_value)\n",
        "\n",
        "            change_amount = abs(new_value_by_state[state] - value_by_state[state])\n",
        "            if change_amount > biggest_change:\n",
        "                biggest_change = change_amount\n",
        "\n",
        "        value_by_state = new_value_by_state\n",
        "\n",
        "        if biggest_change < tolerance:\n",
        "            break\n",
        "\n",
        "    return value_by_state\n",
        "\n",
        "\n",
        "def get_greedy_policy(all_states, all_actions, value_by_state, reward_by_state_action, transitions_by_state_action):\n",
        "    best_action_by_state = {}\n",
        "\n",
        "    for state in all_states:\n",
        "        available_actions = get_available_actions(state, all_actions, reward_by_state_action, transitions_by_state_action)\n",
        "\n",
        "        if len(available_actions) == 0:\n",
        "            best_action_by_state[state] = \"\"\n",
        "            continue\n",
        "\n",
        "        best_action = None\n",
        "        best_q_value = None\n",
        "\n",
        "        for action in available_actions:\n",
        "            immediate_reward = float(reward_by_state_action.get((state, action), 0.0))\n",
        "\n",
        "            expected_future_value = 0.0\n",
        "            for next_state, probability in transitions_by_state_action.get((state, action), []):\n",
        "                expected_future_value += probability * value_by_state[next_state]\n",
        "\n",
        "            q_value = immediate_reward + expected_future_value\n",
        "\n",
        "            if best_q_value is None or q_value > best_q_value or (q_value == best_q_value and action < best_action):\n",
        "                best_q_value = q_value\n",
        "                best_action = action\n",
        "\n",
        "        best_action_by_state[state] = best_action\n",
        "\n",
        "    return best_action_by_state\n",
        "\n",
        "\n",
        "def run_value_iteration_fixed_policy(all_states, policy_action_by_state, reward_by_state_action, transitions_by_state_action):\n",
        "    value_by_state = {state: 0.0 for state in all_states}\n",
        "\n",
        "    max_iterations = 200000\n",
        "    tolerance = 0.0000000001\n",
        "\n",
        "    for _ in range(max_iterations):\n",
        "        biggest_change = 0.0\n",
        "        new_value_by_state = {}\n",
        "\n",
        "        for state in all_states:\n",
        "            action = policy_action_by_state.get(state, \"\")\n",
        "\n",
        "            immediate_reward = float(reward_by_state_action.get((state, action), 0.0))\n",
        "\n",
        "            expected_future_value = 0.0\n",
        "            for next_state, probability in transitions_by_state_action.get((state, action), []):\n",
        "                expected_future_value += probability * value_by_state[next_state]\n",
        "\n",
        "            updated_value = immediate_reward + expected_future_value\n",
        "            new_value_by_state[state] = updated_value\n",
        "\n",
        "            change_amount = abs(updated_value - value_by_state[state])\n",
        "            if change_amount > biggest_change:\n",
        "                biggest_change = change_amount\n",
        "\n",
        "        value_by_state = new_value_by_state\n",
        "\n",
        "        if biggest_change < tolerance:\n",
        "            break\n",
        "\n",
        "    return value_by_state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8oSLkMqvMFF"
      },
      "source": [
        "## Part 1: Evaluate a Do Nothing Plan\n",
        "\n",
        "One of the treatment actions is to do nothing.\n",
        "Calculate the expected discomfort (not rewards) of a policy that always does nothing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvG4mi_sAF9A"
      },
      "source": [
        "Hint: for this value calculation and later ones, use value iteration.\n",
        "The analytical solution has difficulties in practice when there is no discount factor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oji9gHEk8ytE"
      },
      "source": [
        "Save the expected discomfort by state to a file \"do-nothing-discomfort.tsv\" with columns state and expected_discomfort."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-8sGANC-Dzs"
      },
      "source": [
        "Submit \"do-nothing-discomfort.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        state  expected_discomfort\n",
            "0   exposed-1             3.413333\n",
            "1   exposed-2             4.266667\n",
            "2   exposed-3             5.333333\n",
            "3   recovered            -0.000000\n",
            "4  symptoms-1             6.666667\n"
          ]
        }
      ],
      "source": [
        "all_states, all_actions, reward_by_state_action, transitions_by_state_action = build_lookups(rewards_df, transitions_df)\n",
        "\n",
        "do_nothing_policy = {}\n",
        "for state in all_states:\n",
        "    available_actions = get_available_actions(state, all_actions, reward_by_state_action, transitions_by_state_action)\n",
        "    do_nothing_policy[state] = \"do-nothing\" if \"do-nothing\" in available_actions else \"\"\n",
        "\n",
        "do_nothing_values = run_value_iteration_fixed_policy(\n",
        "    all_states,\n",
        "    do_nothing_policy,\n",
        "    reward_by_state_action,\n",
        "    transitions_by_state_action\n",
        ")\n",
        "\n",
        "rows = []\n",
        "for state in sorted(all_states):\n",
        "    rows.append({\"state\": state, \"expected_discomfort\": -float(do_nothing_values[state])})\n",
        "\n",
        "do_nothing_discomfort_df = pd.DataFrame(rows)\n",
        "do_nothing_discomfort_df.to_csv(\"do-nothing-discomfort.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "print(do_nothing_discomfort_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ1ietVp9BCS"
      },
      "source": [
        "## Part 2: Compute an Optimal Treatment Plan\n",
        "\n",
        "Compute an optimal treatment plan for Twizzleflu.\n",
        "It should minimize the expected discomfort (maximize the rewards)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRcByl1h9nBf"
      },
      "source": [
        "Save the optimal actions for each state to a file \"minimum-discomfort-actions.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr00MlhL-Hdv"
      },
      "source": [
        "Submit \"minimum-discomfort-actions.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        state      action\n",
            "0   exposed-1     sleep-8\n",
            "1   exposed-2     sleep-8\n",
            "2   exposed-3     sleep-8\n",
            "3   recovered  do-nothing\n",
            "4  symptoms-1    drink-oj\n"
          ]
        }
      ],
      "source": [
        "all_states, all_actions, reward_by_state_action, transitions_by_state_action = build_lookups(rewards_df, transitions_df)\n",
        "\n",
        "optimal_values = run_value_iteration_optimal(\n",
        "    all_states,\n",
        "    all_actions,\n",
        "    reward_by_state_action,\n",
        "    transitions_by_state_action\n",
        ")\n",
        "\n",
        "optimal_policy = get_greedy_policy(\n",
        "    all_states,\n",
        "    all_actions,\n",
        "    optimal_values,\n",
        "    reward_by_state_action,\n",
        "    transitions_by_state_action\n",
        ")\n",
        "\n",
        "rows = []\n",
        "for state in sorted(all_states):\n",
        "    rows.append({\"state\": state, \"action\": optimal_policy[state]})\n",
        "\n",
        "minimum_discomfort_actions_df = pd.DataFrame(rows)\n",
        "minimum_discomfort_actions_df.to_csv(\"minimum-discomfort-actions.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "print(minimum_discomfort_actions_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65p3NRTy9xjT"
      },
      "source": [
        "## Part 3: Expected Discomfort\n",
        "\n",
        "Using your previous optimal policy, compute the expected discomfort for each state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er6-0c0f-BGw"
      },
      "source": [
        "Save your results in a file \"minimum-discomfort-values.tsv\" with columns state and expected_discomfort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        state  expected_discomfort\n",
            "0   exposed-1                 0.75\n",
            "1   exposed-2                 1.50\n",
            "2   exposed-3                 3.00\n",
            "3   recovered                -0.00\n",
            "4  symptoms-1                 6.00\n"
          ]
        }
      ],
      "source": [
        "all_states, all_actions, reward_by_state_action, transitions_by_state_action = build_lookups(rewards_df, transitions_df)\n",
        "\n",
        "actions_df = pd.read_csv(\"minimum-discomfort-actions.tsv\", sep=\"\\t\")\n",
        "policy_action_by_state = dict(zip(actions_df[\"state\"], actions_df[\"action\"]))\n",
        "\n",
        "policy_values = run_value_iteration_fixed_policy(\n",
        "    all_states,\n",
        "    policy_action_by_state,\n",
        "    reward_by_state_action,\n",
        "    transitions_by_state_action\n",
        ")\n",
        "\n",
        "rows = []\n",
        "for state in sorted(all_states):\n",
        "    rows.append({\"state\": state, \"expected_discomfort\": -float(policy_values[state])})\n",
        "\n",
        "minimum_discomfort_values_df = pd.DataFrame(rows)\n",
        "minimum_discomfort_values_df.to_csv(\"minimum-discomfort-values.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "print(minimum_discomfort_values_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83wnFZfk-UDd"
      },
      "source": [
        "Submit \"minimum-discomfort-values.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKUTt9gx-XBF"
      },
      "source": [
        "## Part 4: Minimizing Twizzleflu Duration\n",
        "\n",
        "Modifiy the Markov decision process to minimize the days until the Twizzle flu is over.\n",
        "To do so, change the reward function to always be -1 if the current state corresponds to being sick (must have symptoms, exposed does not count) and 0 otherwise.\n",
        "To be clear, the action does not matter for this reward function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je9Rt239-uRl"
      },
      "source": [
        "Save your new reward function in a file \"duration-rewards.tsv\" in the same format as \"twizzleflu-rewards.tsv\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       action       state  reward\n",
            "0  do-nothing   exposed-1     0.0\n",
            "1  do-nothing   exposed-2     0.0\n",
            "2  do-nothing   exposed-3     0.0\n",
            "3  do-nothing  symptoms-1    -1.0\n",
            "4  do-nothing  symptoms-2    -1.0\n"
          ]
        }
      ],
      "source": [
        "duration_rewards_df = rewards_df.copy()\n",
        "duration_rewards_df[\"reward\"] = duration_rewards_df[\"state\"].apply(\n",
        "    lambda state: -1.0 if \"symptoms\" in str(state) else 0.0\n",
        ")\n",
        "\n",
        "duration_rewards_df.to_csv(\"duration-rewards.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "print(duration_rewards_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0lubs9v-5XQ"
      },
      "source": [
        "Submit \"duration-rewards.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf73YFzB-802"
      },
      "source": [
        "## Part 5: Optimize for Shorter Twizzleflu\n",
        "\n",
        "Compute an optimal policy to minimize the duration of Twizzleflu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px1xDndA_F3O"
      },
      "source": [
        "Save the optimal actions for each state to a file \"minimum-duration-actions.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        state      action\n",
            "0   exposed-1     sleep-8\n",
            "1   exposed-2     sleep-8\n",
            "2   exposed-3     sleep-8\n",
            "3   recovered  do-nothing\n",
            "4  symptoms-1  do-nothing\n"
          ]
        }
      ],
      "source": [
        "duration_rewards_df = pd.read_csv(\"duration-rewards.tsv\", sep=\"\\t\")\n",
        "\n",
        "all_states, all_actions, reward_by_state_action, transitions_by_state_action = build_lookups(duration_rewards_df, transitions_df)\n",
        "\n",
        "optimal_duration_values = run_value_iteration_optimal(\n",
        "    all_states,\n",
        "    all_actions,\n",
        "    reward_by_state_action,\n",
        "    transitions_by_state_action\n",
        ")\n",
        "\n",
        "optimal_duration_policy = get_greedy_policy(\n",
        "    all_states,\n",
        "    all_actions,\n",
        "    optimal_duration_values,\n",
        "    reward_by_state_action,\n",
        "    transitions_by_state_action\n",
        ")\n",
        "\n",
        "rows = []\n",
        "for state in sorted(all_states):\n",
        "    rows.append({\"state\": state, \"action\": optimal_duration_policy[state]})\n",
        "\n",
        "minimum_duration_actions_df = pd.DataFrame(rows)\n",
        "minimum_duration_actions_df.to_csv(\"minimum-duration-actions.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "print(minimum_duration_actions_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itPVLMaM_UDn"
      },
      "source": [
        "Submit \"minimum-duration-actions.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOzSQ3fV_XBO"
      },
      "source": [
        "## Part 6: Shorter Twizzleflu?\n",
        "\n",
        "Compute the expected number of days sick for each state to a file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zf8j6_D_hbZ"
      },
      "source": [
        "Save the expected sick days for each state to a file \"minimum-duration-days.tsv\" with columns state and expected_sick_days."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        state  expected_sick_days\n",
            "0   exposed-1                1.25\n",
            "1   exposed-2                2.50\n",
            "2   exposed-3                5.00\n",
            "3   recovered               -0.00\n",
            "4  symptoms-1               10.00\n"
          ]
        }
      ],
      "source": [
        "duration_rewards_df = pd.read_csv(\"duration-rewards.tsv\", sep=\"\\t\")\n",
        "actions_df = pd.read_csv(\"minimum-duration-actions.tsv\", sep=\"\\t\")\n",
        "\n",
        "policy_action_by_state = dict(zip(actions_df[\"state\"], actions_df[\"action\"]))\n",
        "\n",
        "all_states, all_actions, reward_by_state_action, transitions_by_state_action = build_lookups(duration_rewards_df, transitions_df)\n",
        "\n",
        "policy_values = run_value_iteration_fixed_policy(\n",
        "    all_states,\n",
        "    policy_action_by_state,\n",
        "    reward_by_state_action,\n",
        "    transitions_by_state_action\n",
        ")\n",
        "\n",
        "rows = []\n",
        "for state in sorted(all_states):\n",
        "    rows.append({\"state\": state, \"expected_sick_days\": -float(policy_values[state])})\n",
        "\n",
        "minimum_duration_days_df = pd.DataFrame(rows)\n",
        "minimum_duration_days_df.to_csv(\"minimum-duration-days.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "print(minimum_duration_days_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVApozXF_pjI"
      },
      "source": [
        "Submit \"minimum-duration-days.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znw87KK7_uv5"
      },
      "source": [
        "## Part 7: Speed vs Pampering\n",
        "\n",
        "Compute the expected discomfort using the policy to minimize days sick, and compare the results to the expected discomfort when optimizing to minimize discomfort."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3ZVJ2lcAAkP"
      },
      "source": [
        "Save the results to a file \"policy-comparison.tsv\" with columns state, speed_discomfort, and minimize_discomfort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        state  speed_discomfort  minimize_discomfort\n",
            "0   exposed-1          0.833333                 0.75\n",
            "1   exposed-2          1.666667                 1.50\n",
            "2   exposed-3          3.333333                 3.00\n",
            "3   recovered         -0.000000                -0.00\n",
            "4  symptoms-1          6.666667                 6.00\n"
          ]
        }
      ],
      "source": [
        "actions_speed_df = pd.read_csv(\"minimum-duration-actions.tsv\", sep=\"\\t\")\n",
        "actions_min_discomfort_df = pd.read_csv(\"minimum-discomfort-actions.tsv\", sep=\"\\t\")\n",
        "\n",
        "speed_policy = dict(zip(actions_speed_df[\"state\"], actions_speed_df[\"action\"]))\n",
        "min_discomfort_policy = dict(zip(actions_min_discomfort_df[\"state\"], actions_min_discomfort_df[\"action\"]))\n",
        "\n",
        "all_states, all_actions, reward_by_state_action, transitions_by_state_action = build_lookups(rewards_df, transitions_df)\n",
        "\n",
        "speed_values = run_value_iteration_fixed_policy(\n",
        "    all_states,\n",
        "    speed_policy,\n",
        "    reward_by_state_action,\n",
        "    transitions_by_state_action\n",
        ")\n",
        "\n",
        "min_discomfort_values = run_value_iteration_fixed_policy(\n",
        "    all_states,\n",
        "    min_discomfort_policy,\n",
        "    reward_by_state_action,\n",
        "    transitions_by_state_action\n",
        ")\n",
        "\n",
        "rows = []\n",
        "for state in sorted(all_states):\n",
        "    rows.append(\n",
        "        {\n",
        "            \"state\": state,\n",
        "            \"speed_discomfort\": -float(speed_values[state]),\n",
        "            \"minimize_discomfort\": -float(min_discomfort_values[state]),\n",
        "        }\n",
        "    )\n",
        "\n",
        "policy_comparison_df = pd.DataFrame(rows)\n",
        "policy_comparison_df.to_csv(\"policy-comparison.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "print(policy_comparison_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVhLZuuaANNf"
      },
      "source": [
        "Submit \"policy-comparison.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smsTLuFcvR-I"
      },
      "source": [
        "## Part 8: Code\n",
        "\n",
        "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi8lV2pbvWMs"
      },
      "source": [
        "## Part 9: Acknowledgements\n",
        "\n",
        "If you discussed this assignment with anyone, please acknowledge them here.\n",
        "If you did this assignment completely on your own, simply write none below.\n",
        "\n",
        "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for. If you did not use any other libraries, simply write none below.\n",
        "\n",
        "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy. If you did not use any generative AI tools, simply write none below."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": false
    },
    "kernelspec": {
      "display_name": "ds-gpu (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
